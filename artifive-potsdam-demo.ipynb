{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3214be-fa08-4c47-b53b-913f2e73c9d8",
   "metadata": {},
   "source": [
    "# ARTIFIVE-POTSDAM DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e26c94-5021-4edb-ad2b-0e6b5803d0c1",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdB23YiUA02F"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d079a68-8205-4943-a8dc-4751b99c7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch as th\n",
    "import torchvision as tv\n",
    "\n",
    "from imagedataset import ImageDataset, collate_fn\n",
    "from open_turbo_jpeg import open_turbo_jpeg\n",
    "from transformations import Resize, StandardNormalize, Compose, denormalize\n",
    "import shapely\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from visualization import draw_rectangles, draw_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98465063-f482-49db-b8c7-2fe5aeac117a",
   "metadata": {},
   "source": [
    "# dataset paths\n",
    "\n",
    "* adjust accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a94c20-1502-4b69-94cc-559d0024fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "potsdam_training_path = Path(\"/fastdata/artifive-potsdam/v1/patched/600x600_overlap200/training/\")\n",
    "potsdam_test_path = Path(\"/fastdata/artifive-potsdam/v1/patched/600x600/test/\")\n",
    "sample_filename = \"annotations*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc798e-1933-44e1-8973-d4bc19847a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "art_path = Path(\"/fastdata/artifive-potsdam/v1/artificial/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ffbea-e60a-4c58-9591-ad9e6b130539",
   "metadata": {},
   "source": [
    "# prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482bfe3-f183-4fcf-8459-256539fca9e3",
   "metadata": {},
   "source": [
    "## sample preprocessor\n",
    "\n",
    "can modify samples when creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b34cb-8a0e-4f2f-a34e-845de009c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = 20\n",
    "max_size = 200\n",
    "min_either_size = 40 # at least one side needs to be larger\n",
    "\n",
    "def sample_preprocessor(sample):\n",
    "    # this removes polygons whose sizes are outside of the specified limits\n",
    "    # it is applied as a preprocessing step\n",
    "    polys = sample[\"annotations\"][\"polygons\"] \n",
    "    if len(polys):\n",
    "        polys_tensored = th.tensor([p.bounds for p in polys])\n",
    "        sizes = polys_tensored[:,2:] - polys_tensored[:, :2]\n",
    "        keeper = ((sizes > min_size) & (sizes < max_size)).all(-1)\n",
    "        keeper = keeper & (sizes > min_either_size).any(-1)\n",
    "        for k, vs in sample[\"annotations\"].items():\n",
    "            sample[\"annotations\"][k] = [v for v, keep in zip(vs, keeper) if keep]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a3b688-7bfa-4353-b1ef-35eeed7d4a0c",
   "metadata": {},
   "source": [
    "## sample pre filters\n",
    "\n",
    "can remove samples when creating dataset; is called after sample preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f1087-8244-42fd-800b-8543de620163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(sample):\n",
    "    return len(sample[\"annotations\"][\"labels\"]) > 0\n",
    "\n",
    "def remove_nonempty(sample):\n",
    "    return len(sample[\"annotations\"][\"labels\"]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b816548-8400-4869-911c-1226af5a4ba0",
   "metadata": {},
   "source": [
    "## polygon to axis aligned bounding box transformation\n",
    "\n",
    "* since the annotations contain object polygons they need to be converted to suitable format\n",
    "* here we convert these shapely polygons to axis aligned bounding boxes\n",
    "* it is called in the transformation pipeline\n",
    "* a similar function needs to be designed if one wants to use object aligned bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c447ce8-38ef-42d6-964a-4b785dd48d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __clip_polys(polys, image_size):\n",
    "    box = shapely.geometry.box(0, 0, image_size[2] - 1, image_size[1] - 1)\n",
    "    return [p.intersection(box) for p in polys]\n",
    "\n",
    "def __remove_objects(objects, drop_list):\n",
    "    for key, values in objects.items():\n",
    "        objects[key] = [v for v, drop in zip(values, drop_list) if not drop]\n",
    "    return objects\n",
    "    \n",
    "def polys_to_axisaligned_boxes(input, target):\n",
    "    if \"polygons\" in target:\n",
    "        target = target.copy()\n",
    "        target[\"polygons\"] = __clip_polys(target[\"polygons\"], input[\"image\"].shape)\n",
    "        drop_list = [p.is_empty for p in target[\"polygons\"]]\n",
    "        target = __remove_objects(target, drop_list) \n",
    "        polygons = target.pop(\"polygons\")\n",
    "        boxes = [p.bounds for p in polygons]\n",
    "        target[\"boxes\"] = boxes\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b75647-7a52-4113-9973-bf94b3040381",
   "metadata": {},
   "source": [
    "## prepare transformation pipeline\n",
    "\n",
    "* downsampling to maximal 300 x 300 pixel\n",
    "* the collate function pads batch samples if required\n",
    "* image normalization to ImageNet stats\n",
    "* polygon to bounding box conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd787c-c9b6-45a4-b3b9-2624709738f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stats_imagenet = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "data_mean = data_stats_imagenet[0]\n",
    "data_std = data_stats_imagenet[1]\n",
    "\n",
    "transforms = Compose([\n",
    "    Resize(300, max_size=300),\n",
    "    StandardNormalize(data_mean, data_std),\n",
    "    polys_to_axisaligned_boxes, \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ae2e9-7172-4de8-bca8-e51468ed1682",
   "metadata": {},
   "source": [
    "## create dataset and dataloader\n",
    "\n",
    "* here we use a libjpeg-turbo accelerated image loading function\n",
    "* optional segmentations are provided for the Potsdam part of the dataset, but not for the artificial part\n",
    "* for fusion of real and artificial images resort to pytorchs utilities like torch.utils.data.ConcatDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0d146-e1ce-4e73-9c00-b72ca0c92963",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(\n",
    "    potsdam_training_path, \n",
    "    sample_filename=sample_filename, \n",
    "    transform=transforms,\n",
    "    sample_preprocessor=sample_preprocessor,\n",
    "    sample_filter=remove_empty,\n",
    "    add_segmentation=False,\n",
    "    image_loading_fn=open_turbo_jpeg,\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 8\n",
    "shuffle = True\n",
    "dataloader = th.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=collate_fn, shuffle=shuffle)\n",
    "\n",
    "len(dataset), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43349ac-004c-4141-986d-f32a7a146f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_sample_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a44f22-2d38-4eff-9a78-4d365856c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_target_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c458c3-b5b5-44c1-a281-e4019af7c72d",
   "metadata": {},
   "source": [
    "# single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740e704-6004-45a3-9bc2-6eaca020250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target = dataset[-1]\n",
    "image = denormalize(input[\"image\"], data_mean, data_std)\n",
    "\n",
    "f, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axs[0].imshow(image.permute(1, 2, 0).clip(0, 1))\n",
    "c = axs[1].imshow(input[\"mask\"].permute(1, 2, 0))\n",
    "c.set_clim(0, 1)\n",
    "for ax in axs:\n",
    "    ax.axis(False)\n",
    "\n",
    "input.keys(), target.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107d43c-c369-4e7b-ae83-9543fd1ee1c9",
   "metadata": {},
   "source": [
    "# single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131293f-bfe2-4fa4-8a19-6df53699e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used for plotting\n",
    "inverse_label_map = {v: k for k, v in dataset.label_map.items()}\n",
    "inverse_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb51c983-d4b3-4e09-a500-fe3ce5cf95f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target = next(iter(dataloader))\n",
    "images = input[\"image\"]\n",
    "images = denormalize(images, data_mean, data_std)\n",
    "\n",
    "n_cols = 8\n",
    "padding = 2\n",
    "\n",
    "gridded_images = tv.utils.make_grid(images, n_cols, padding)\n",
    "gridded_masks = tv.utils.make_grid(input[\"mask\"], n_cols, padding)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.imshow(gridded_images.permute(1, 2, 0))\n",
    "ax.axis(False);\n",
    "ax.set_title(\"rgb + bounding boxes\");\n",
    "\n",
    "_, h, w = images[0].shape\n",
    "j = -1\n",
    "for i, (bxs, lbls) in enumerate(zip(target[\"boxes\"], target[\"labels\"])):\n",
    "    bxs = bxs.clone()\n",
    "    bxs[:,::2] = bxs[:,::2] + (w + padding) * (i % n_cols)\n",
    "    if i % n_cols == 0:\n",
    "        j += 1\n",
    "    bxs[:,1::2] = bxs[:,1::2] + (h + padding) * j\n",
    "    draw_rectangles(ax, bxs)\n",
    "    lbls = [inverse_label_map[lb] for lb in lbls.tolist()]\n",
    "    draw_texts(ax, lbls, bxs[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc406de-95e0-45b8-95c3-9ab8e95a60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.imshow(gridded_masks.permute(1, 2, 0))\n",
    "ax.axis(False);\n",
    "ax.set_title(\"masks\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev_pytorch19]",
   "language": "python",
   "name": "conda-env-dev_pytorch19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
